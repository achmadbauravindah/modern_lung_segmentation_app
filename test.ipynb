{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAAFLklEQVR4nO2d25qdIAyFY7++N/HJ7UXnuEeFHAnO+m+6TxCyTAKiTokAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAB4BFueqUa051kbJUmA4/0FD2vQ3v4NFi1DgOPbuzEJ2pfXoRIEC9CI+PWzAQXay/tACSIFOPOeiIh7Dr36T0RhIvyN6ZaIqJ27T8QXn382JDrTLkSCuAhofP3dbRY0ovPQGS+gAv4E9ElE9/5fBPkXThvz0WumIEqAe//vvmw337O/AkEC3Pvf47oxH5dflaIdPa4PZes0dg6CkAiwHf8OzmkQIcDBAZ1+Uj4NuuF/HMdx7UQ/fW4ay/GPAOPohgLcUQF3Acxj4xQrH3gLkJWfbpXQWYC0+sRZhmSMFLBeHTN3MJNx/+0COCWB69mg5KhcGh7uxGfonjXAJyp59Ic+IeAoQMC56i0Bp4Y2BAXgNoMd6ogAvwhwK8s8/EuPEAjbEcqAHfpwE8BvXhbs/DmEQMUI4IBfhiOqgL365VJLB/FaCEkz4NauoDPz+J1SwHdlnnjJelYNYNPXXzDngI/Y4gDomR3v0OpAxVmASpX3EZp0DugfYMeuEhD73x91W0mA8cG+M1C6hjuN96+L2P+hQY8qUOCkOEaAYQVsgy86CyyFPAAGD9pgCNhG/4AImF4EwiIgJQTsEaCwz2ajfpROgZ3jbUwRwPl2N1MRKB0BS6AogYIjFl8Ff30EmDdEFPILbI71bnFiQgSw+28tVdAqQIWzURPFa0D8Q0bFBRiD55mOnQSHDRiKwCMiwEJ1ATjaQHUBwrEJMH0v4g3WN7UJYDDsi/5IpKdAyKNfBqrXgHC5qgsQjkmA5U8E6DERwHPMhi+Ex22ofSgfATz2M/U8WF6A6GnAIsATamB2BFRbBi2QAtEFvr4AwTGTLEC5DDCRsAogGr1Irp0HDRGQNQnERk39GlBo1+EFeQZoY2YkCSakgBxWtou8UWKFFKiKPAP0O3cDSZC/QSsXwGCsr4BSAH0K5CoeVgb0ArDfIGayTBGMCoFMAdjUOuOmQRGpNZCIuoVQV5SWSQEKigG1ADP2wyIUWCkCQhRIFIAd+vA/NU4UwGXwd/dEqqrgWilA/g9WawWQ10BWWorq543lIuC6EF59fs96AhRZEuavA/vGaxdBduzLsxAqBZh8fxz7dbVgDaDLMqA5LEoBzgeQh18hTIsA70UsO/WTJQA793emp8pGlgD+ZzHs082aRZCIdicFdAKIyy2rzNRl5jLwg59/hz9xGnwOCwvgUwUWFoBcSotKgCI18MfUqjGjEkBjKAb+/lZRBddOAQeWFmB3CMalBfBAI0CVpwWJyB4Ca0fATmYFNALITcpbKJEH59oRQERWdR0F4O0d9us0HB8Btm3bts+F2Z6rAZ+8GsZDgO1kn37fNmaHvjvYN5oUAryc3J+5T0RE+75CLlgj4NJ9IiJaQALj3w/oXqSqcSXzBpMA20AKZgSBxYJcgI+1Rv/w/2ff7vPExOshEK+EDPcKV3wCjMUt5AK82SjpvwJtBDzFf/XV4af4LxegERGPlP8krCNRnQ7Xcf8HLG3wV2GisP9yFBFQ23/pQkAswF7Nf+N4HrAjZOMBAthC4AECgO9PU80ezBTabxfgqwKzhzKL9tsF+FBAuBJ6ziyw67adniOA8jGCzP/dMp6DSOrSkyLgaYdTQxMXQfl+QGnk5wXPSoHqmxUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQA7/AMikZSNr+TFLAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=256x256>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import load_img, img_to_array, array_to_img\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "\n",
    "def overlapMask(model_name):\n",
    "    # Get Array Image =\n",
    "    image_to_predict = img_to_array(load_img('./sample_images/image_to_predict.bmp', color_mode='grayscale', target_size=(256, 256))).astype('float32')/255.0\n",
    "    gt_image = np.squeeze(img_to_array(load_img('./sample_images/ground_truth.bmp', color_mode='grayscale', target_size=(256, 256))).astype('float32'))/255.0\n",
    "    # Reshape to model.predict\n",
    "    image_arr_reshape = image_to_predict[np.newaxis, ...]\n",
    "    # Load Model\n",
    "    model = tf.keras.models.load_model('./models/{}/{}.h5'.format(model_name, model_name), compile=False)\n",
    "    # Get Segmentation (Predicted Images)\n",
    "    mask_arr = model.predict(image_arr_reshape, verbose=0)\n",
    "    # To Binary\n",
    "    gt_image = (gt_image > 0.5).astype('float32')\n",
    "    mask_arr = (mask_arr > 0.5).astype('float32')\n",
    "    # Reduce The Dimension\n",
    "    mask_arr = np.squeeze(mask_arr)\n",
    "    # Overlap\n",
    "    gt_image[gt_image != mask_arr] = 0.5\n",
    "    return array_to_img(np.expand_dims(gt_image, axis=2))\n",
    "\n",
    "overlapMask('UNet')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kuliah",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
